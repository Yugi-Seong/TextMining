{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72fe0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e562c",
   "metadata": {},
   "source": [
    "# 텍스트 분석\n",
    "\n",
    "* NLP : 자연어 처리(Natural Laguage Processing)란 기계가 자연어를 이해하고 해석하여 처리할 수 있도록 하는 일    \n",
    "\n",
    "*    \n",
    "텍스트 분류(Text Classification)  \n",
    "감정분석 (Sentiment Analysis)  \n",
    "텍스트 요약 (Summarization)  \n",
    "텍스트 군집화와 유사도 측정  \n",
    "\n",
    "텍스트 분석의 feature의 개수는 사람이 정하기 마련이다!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c69a82",
   "metadata": {},
   "source": [
    "파이썬 기반의 NLP, 텍스트 분석 패키지 : nltk \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51911e5",
   "metadata": {},
   "source": [
    "텍스트 전처리   \n",
    "1. 클렌징 : 불필요한 문자, 기호 등을 사전에 제거   \n",
    "2. 토큰화(Tokenization) : 문장 토근화, 단어 토큰화, n - gram    \n",
    "3. 필터링/ 스톱워드 제거 / 철자 수정 : 불필요한 단어나 분석에 큰 의미가 없는 단어(a, the, is) 수정 잘못된 철자 수정   \n",
    "4. stemming / Lemmatization : 어근(단어 원형)추출, Lemmatization 이 Stemming 보다 정교하고 의미론적 기반에서 단어 원형을 찾아줌    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ef91f",
   "metadata": {},
   "source": [
    "### 문장 토큰화 : sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af80787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize \n",
    "\n",
    "text_sample = '''The Matrix is everywhere its all around us, here in this room.\n",
    "        You can see it out your window or on your television.\n",
    "        You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfffb1e",
   "metadata": {},
   "source": [
    "### 단어 토큰화 : word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d651d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 14\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words),len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36bd0e",
   "metadata": {},
   "source": [
    "### 문장별 단어 토큰화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee7c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#여러 문장들에 대한 단어 토큰화 \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text) :\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #분리된 문장별 단어 토큰화 \n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대한 문장별 단어 토큰화 수행 \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfa215",
   "metadata": {},
   "source": [
    "### Stopwords 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3763d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6d646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "#영어 stopwords 확인 \n",
    "print(\"영어 stop words 갯수 : \", len(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "#영어 stopwords 중 20개만 출력해 보자\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74029dec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "#위 예제 3개 문장에서 얻은 단어 토큰에 대해 stopwords 제거해보자 \n",
    "import nltk \n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop \n",
    "for sentence in word_tokens :\n",
    "    filtered_words = []\n",
    "    #개별 문장별로 tokenize 된 sentence list 에 대해 stop word 제거 Loop \n",
    "    for word in sentence : \n",
    "        # 소문자로 모두 변환합니다.\n",
    "        word = word.lower()\n",
    "        \n",
    "        # tokenize 된 개별 word 가 stop words 들의 단어에 포함되지 않으면 word_tokens 에 추가 \n",
    "        if word not in stopwords :\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b955c2",
   "metadata": {},
   "source": [
    "### Stemming 과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ea611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "#Stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cd64d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happier happy\n",
      "fancier fancy\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier'),lemma.lemmatize('fanciest','a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e08bc",
   "metadata": {},
   "source": [
    "### COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f04113ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1],[0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2252e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "#0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "#sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소행렬 생성 \n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "232071be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31329ae",
   "metadata": {},
   "source": [
    "### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2e6d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "\n",
    "#0 이 아닌 데이터 추출 \n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "#COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "#행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성 \n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "#CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos,row_pos_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4a250e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
    "print(sparse_coo.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01c21a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\")\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "747fbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                 [1, 4, 0, 3, 2, 5],\n",
    "                 [0, 6, 0, 3, 0, 0],\n",
    "                 [2, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 7, 0, 8],\n",
    "                 [1, 0, 0, 0, 0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010e75a",
   "metadata": {},
   "source": [
    "## Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c971b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams ,word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "all_ngrams = ngrams(words, 2)\n",
    "ngrams = [ngram for ngram in all_ngrams]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a624c4",
   "metadata": {},
   "source": [
    "## Bag of Words – BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0648068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.                   You can see it out your window or on your television.                   You feel it when you go to work, or go to church or pay your taxes.', 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "text_sample_01 = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "                  You can see it out your window or on your television. \\\n",
    "                  You feel it when you go to work, or go to church or pay your taxes.'\n",
    "text_sample_02 = 'You take the blue pill and the story ends.  You wake in your bed and you believe whatever you want to believe\\\n",
    "                  You take the red pill and you stay in Wonderland and I show you how deep the rabbit-hole goes.'\n",
    "text=[]\n",
    "text.append(text_sample_01); text.append(text_sample_02)\n",
    "print(text,\"\\n\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d39c8",
   "metadata": {},
   "source": [
    "CountVectorizer객체 생성 후 fit(), transform()으로 텍스트에 대한 feature vectorization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff23d0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcc856d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_vect = cnt_vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0a13a",
   "metadata": {},
   "source": [
    "피처 벡터화 후 데이터 유형 및 여러 속성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40e051dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 51)\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t3\n",
      "  (0, 25)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 40)\t2\n",
      "  :\t:\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 18)\t2\n",
      "  (1, 27)\t2\n",
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 35)\t2\n",
      "  (1, 38)\t4\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 43)\t1\n",
      "  (1, 44)\t1\n",
      "  (1, 47)\t1\n",
      "  (1, 49)\t7\n",
      "  (1, 50)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(ftr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bbf6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 38, 'matrix': 22, 'is': 19, 'everywhere': 11, 'its': 21, 'all': 0, 'around': 2, 'us': 41, 'here': 15, 'even': 10, 'in': 18, 'this': 39, 'room': 30, 'you': 49, 'can': 6, 'see': 31, 'it': 20, 'out': 25, 'your': 50, 'window': 46, 'or': 24, 'on': 23, 'television': 37, 'feel': 12, 'when': 45, 'go': 13, 'to': 40, 'work': 48, 'church': 7, 'pay': 26, 'taxes': 36, 'take': 35, 'blue': 5, 'pill': 27, 'and': 1, 'story': 34, 'ends': 9, 'wake': 42, 'bed': 3, 'believe': 4, 'whatever': 44, 'want': 43, 'red': 29, 'stay': 33, 'wonderland': 47, 'show': 32, 'how': 17, 'deep': 8, 'rabbit': 28, 'hole': 16, 'goes': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b69eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 5)\n",
      "{'window': 4, 'pill': 1, 'wake': 2, 'believe': 0, 'want': 3}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(max_features=5, stop_words='english')\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcb8e3",
   "metadata": {},
   "source": [
    "ngram_range 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64978531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (2, 125)\n",
      "{'the': 82, 'matrix': 49, 'is': 42, 'everywhere': 25, 'its': 47, 'all': 0, 'around': 6, 'us': 94, 'here': 32, 'even': 23, 'in': 38, 'this': 88, 'room': 67, 'you': 110, 'can': 15, 'see': 69, 'it': 44, 'out': 57, 'your': 120, 'window': 104, 'or': 53, 'on': 51, 'television': 80, 'feel': 27, 'when': 102, 'go': 29, 'to': 90, 'work': 108, 'church': 17, 'pay': 59, 'taxes': 79, 'the matrix': 84, 'matrix is': 50, 'is everywhere': 43, 'everywhere its': 26, 'its all': 48, 'all around': 1, 'around us': 7, 'us here': 95, 'here even': 33, 'even in': 24, 'in this': 39, 'this room': 89, 'room you': 68, 'you can': 112, 'can see': 16, 'see it': 70, 'it out': 45, 'out your': 58, 'your window': 124, 'window or': 105, 'or on': 55, 'on your': 52, 'your television': 123, 'television you': 81, 'you feel': 113, 'feel it': 28, 'it when': 46, 'when you': 103, 'you go': 114, 'go to': 30, 'to work': 93, 'work or': 109, 'or go': 54, 'to church': 92, 'church or': 18, 'or pay': 56, 'pay your': 60, 'your taxes': 122, 'take': 77, 'blue': 13, 'pill': 61, 'and': 2, 'story': 75, 'ends': 21, 'wake': 96, 'bed': 8, 'believe': 10, 'whatever': 100, 'want': 98, 'red': 65, 'stay': 73, 'wonderland': 106, 'show': 71, 'how': 36, 'deep': 19, 'rabbit': 63, 'hole': 34, 'goes': 31, 'you take': 117, 'take the': 78, 'the blue': 83, 'blue pill': 14, 'pill and': 62, 'and the': 4, 'the story': 87, 'story ends': 76, 'ends you': 22, 'you wake': 118, 'wake in': 97, 'in your': 41, 'your bed': 121, 'bed and': 9, 'and you': 5, 'you believe': 111, 'believe whatever': 11, 'whatever you': 101, 'you want': 119, 'want to': 99, 'to believe': 91, 'believe you': 12, 'the red': 86, 'red pill': 66, 'you stay': 116, 'stay in': 74, 'in wonderland': 40, 'wonderland and': 107, 'and show': 3, 'show you': 72, 'you how': 115, 'how deep': 37, 'deep the': 20, 'the rabbit': 85, 'rabbit hole': 64, 'hole goes': 35}\n"
     ]
    }
   ],
   "source": [
    "cnt_vect = CountVectorizer(ngram_range=(1,2))\n",
    "cnt_vect.fit(text)\n",
    "ftr_vect = cnt_vect.transform(text)\n",
    "print(type(ftr_vect), ftr_vect.shape)\n",
    "print(cnt_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb73ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
